% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{han2023survey}
K.~Han, Y.~Wang, H.~Chen, X.~Chen, J.~Guo, Z.~Liu, Y.~Tang, A.~Xiao, C.~Xu,
  Y.~Xu \emph{et~al.}, ``A survey on vision transformer,'' \emph{IEEE
  Transactions on Pattern Analysis and Machine Intelligence}, vol.~45, no.~1,
  pp. 87--110, 2023.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou,
  ``Training data-efficient image transformers \& distillation through
  attention,'' in \emph{International conference on machine learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 10\,347--10\,357.

\bibitem{he2022masked}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick, ``Masked
  autoencoders are scalable vision learners,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2022, pp.
  16\,000--16\,009.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Advances in neural information processing systems}, 2017, pp.
  5998--6008.

\bibitem{flowers_dataset}
A.~Mamaev, ``Flowers recognition dataset,''
  \url{https://www.kaggle.com/alxmamaev/flowers-recognition}, 2018, accessed:
  2025-11-20.

\bibitem{wang2024flower}
Y.~Wang, ``Flower classification and key parameter analysis based on vit,'' in
  \emph{Proceedings of the 2024 International Conference on Image Processing
  and Media Computing (ICIPMC)}.\hskip 1em plus 0.5em minus 0.4em\relax
  SciTePress, 2024.

\end{thebibliography}
